{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick-Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section, we understand minimal and sufficient usage of `opcc` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-19T17:52:56.428425Z",
     "iopub.status.busy": "2023-10-19T17:52:56.428220Z",
     "iopub.status.idle": "2023-10-19T17:52:58.264010Z",
     "shell.execute_reply": "2023-10-19T17:52:58.263236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import opcc\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-10-19T17:52:58.267829Z",
     "iopub.status.busy": "2023-10-19T17:52:58.267132Z",
     "iopub.status.idle": "2023-10-19T17:52:58.297643Z",
     "shell.execute_reply": "2023-10-19T17:52:58.296946Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetah-v2'\n",
    "dataset_name = 'random'\n",
    "\n",
    "# ########################################################\n",
    "# Policy Comparison Queries (PCQ) (Section : 3.1 in paper)\n",
    "# ########################################################\n",
    "# Queries are dictionaries with policies as keys and\n",
    "# corresponding queries as values.\n",
    "queries = opcc.get_queries(env_name)\n",
    "\n",
    "\n",
    "# ########################################################\n",
    "# Following is a random answer predictor for a query\n",
    "# ########################################################\n",
    "def random_predictor(obs_a, obs_b, action_a, action_b,\n",
    "                     policy_a, policy_b, horizon):\n",
    "    answer = np.random.randint(low=0, high=2, size=len(obs_a)).tolist()\n",
    "    confidence = np.random.rand(len(obs_a)).tolist()\n",
    "    return answer, confidence\n",
    "\n",
    "# ########################################################\n",
    "# Query Iterator\n",
    "# ########################################################\n",
    "targets = []\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "# Batch iteration through Queries :\n",
    "for (policy_a_id, policy_b_id), query_batch in queries.items():\n",
    "\n",
    "    # retrieve policies\n",
    "    policy_a, _ = opcc.get_policy(*policy_a_id)\n",
    "    policy_b, _ = opcc.get_policy(*policy_b_id)\n",
    "\n",
    "    # query-a\n",
    "    obs_a = query_batch['obs_a']\n",
    "    action_a = query_batch['action_a']\n",
    "\n",
    "    # query-b\n",
    "    obs_b = query_batch['obs_b']\n",
    "    action_b = query_batch['action_b']\n",
    "\n",
    "    # horizon for policy evaluation\n",
    "    horizon = query_batch['horizon']\n",
    "\n",
    "    # ground truth binary vector:\n",
    "    # (Q(obs_a, action_a, policy_a, horizon)\n",
    "    # <  Q(obs_b, action_b, policy_b, horizon))\n",
    "    target = query_batch['target'].tolist()\n",
    "    targets += target\n",
    "\n",
    "    # Let's make predictions for the given queries.\n",
    "    # One can use any mechanism to predict the corresponding\n",
    "    # answer to queries, and we simply use a random predictor\n",
    "    # over here for demonstration purposes\n",
    "    p, c = random_predictor(obs_a, obs_b, action_a, action_b,\n",
    "                            policy_a, policy_b, horizon)\n",
    "    predictions += p\n",
    "    confidences += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the query `predictions` and `targets`, we demo estimation of various evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-19T17:52:58.301299Z",
     "iopub.status.busy": "2023-10-19T17:52:58.300694Z",
     "iopub.status.idle": "2023-10-19T17:52:58.335725Z",
     "shell.execute_reply": "2023-10-19T17:52:58.335079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aurcc: 0.5011618812974469, rpp: 0.12899288888888888, cr_10:1.0\n"
     ]
    }
   ],
   "source": [
    "# #########################################\n",
    "# (Section 3.3 in paper)\n",
    "# #########################################\n",
    "loss = np.logical_xor(predictions, targets)  # we use 0-1 loss for demo\n",
    "\n",
    "# List of tuples (coverage, selective_risks, tau)\n",
    "coverage_sr_tau = []\n",
    "tau_interval=0.01\n",
    "for tau in np.arange(0, 1 + 2 * tau_interval, tau_interval):\n",
    "  non_abstain_filter = confidences >= tau\n",
    "  if any(non_abstain_filter):\n",
    "    selective_risk = np.sum(loss[non_abstain_filter])\n",
    "    selective_risk /= np.sum(non_abstain_filter)\n",
    "    coverage = np.mean(non_abstain_filter)\n",
    "    coverage_sr_tau.append((coverage, selective_risk, tau))\n",
    "  else:\n",
    "    # 0 risk for 0 coverage\n",
    "    coverage_sr_tau.append((0, 0, tau))\n",
    "\n",
    "coverages, selective_risks, taus = list(zip(*sorted(coverage_sr_tau)))\n",
    "assert selective_risks[0] == 0 and coverages[0] == 0 , \"no coverage not found\"\n",
    "assert coverages[-1] == 1, 'complete coverage not found'\n",
    "\n",
    "# AURCC ( Area Under Risk-Coverage Curve): Ideally, we would like it to be 0\n",
    "aurcc = metrics.auc(x=coverages,y=selective_risks)\n",
    "\n",
    "# Reverse-pair-proportion\n",
    "rpp = np.logical_and(np.expand_dims(loss, 1)\n",
    "                     < np.expand_dims(loss, 1).transpose(),\n",
    "                     np.expand_dims(confidences, 1)\n",
    "                     < np.expand_dims(confidences, 1).transpose()).mean()\n",
    "\n",
    "# Coverage Resolution (cr_k) : Ideally, we would like it to be 1\n",
    "k = 10\n",
    "bins = [_ for _ in np.arange(0, 1, 1 / k)]\n",
    "cr_k = np.unique(np.digitize(coverages, bins)).size / len(bins)\n",
    "\n",
    "# print evaluation metrics\n",
    "print(f\"aurcc: {aurcc}, rpp: {rpp}, cr_{k}:{cr_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-19T17:52:58.339065Z",
     "iopub.status.busy": "2023-10-19T17:52:58.338615Z",
     "iopub.status.idle": "2023-10-19T17:53:14.512891Z",
     "shell.execute_reply": "2023-10-19T17:53:14.512195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/halfcheetah_random-v2.hdf5 to /home/runner/.d4rl/datasets/halfcheetah_random-v2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:  11%|█         | 1/9 [00:00<00:01,  5.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:  33%|███▎      | 3/9 [00:00<00:01,  4.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:  44%|████▍     | 4/9 [00:01<00:01,  3.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:  56%|█████▌    | 5/9 [00:01<00:01,  2.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile:  67%|██████▋   | 6/9 [00:02<00:01,  2.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "load datafile: 100%|██████████| 9/9 [00:02<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ###########################################\n",
    "# Datasets: (Section 4 in paper - step (1) )\n",
    "# ###########################################\n",
    "env_name = 'HalfCheetah-v2'\n",
    "\n",
    "# list all dataset names corresponding to an env\n",
    "dataset_names = opcc.get_dataset_names(env_name)\n",
    "dataset_name = 'random'\n",
    "\n",
    "# This is a very-slim wrapper over D4RL datasets.\n",
    "dataset = opcc.get_qlearning_dataset(env_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-19T17:53:14.516358Z",
     "iopub.status.busy": "2023-10-19T17:53:14.515896Z",
     "iopub.status.idle": "2023-10-19T17:53:14.712145Z",
     "shell.execute_reply": "2023-10-19T17:53:14.711418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Score: 1208.6197452729875\n"
     ]
    }
   ],
   "source": [
    "import opcc, gym, torch\n",
    "env_name = \"HalfCheetah-v2\"\n",
    "policy, policy_info = opcc.get_policy(env_name, pre_trained=1)\n",
    "\n",
    "done = False\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "episode_score=0\n",
    "while not done:\n",
    "    action = policy(torch.tensor(obs).unsqueeze(0))\n",
    "    action = action.data.cpu().numpy()[0].astype('float32')\n",
    "    obs, reward, done, step_info = env.step(action)\n",
    "    episode_score += reward\n",
    "\n",
    "print(f\"Episode Score: {episode_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
