{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick-Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section, we understand minimal and sufficient usage of `opcc` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opcc\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We iterate over queries for an environment and make random predictions for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetah-v2'\n",
    "dataset_name = 'random'\n",
    "\n",
    "# ########################################################\n",
    "# Policy Comparison Queries (PCQ) (Section : 3.1 in paper)\n",
    "# ########################################################\n",
    "# Queries are dictionaries with policies as keys and\n",
    "# corresponding queries as values.\n",
    "queries = opcc.get_queries(env_name)\n",
    "\n",
    "\n",
    "# ########################################################\n",
    "# Following is a random answer predictor for a query\n",
    "# ########################################################\n",
    "def random_predictor(obs_a, obs_b, action_a, action_b,\n",
    "                     policy_a, policy_b, horizon):\n",
    "    answer = np.random.randint(low=0, high=2, size=len(obs_a)).tolist()\n",
    "    confidence = np.random.rand(len(obs_a)).tolist()\n",
    "    return answer, confidence\n",
    "\n",
    "# ########################################################\n",
    "# Query Iterator\n",
    "# ########################################################\n",
    "targets = []\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "# Batch iteration through Queries :\n",
    "for (policy_a_id, policy_b_id), query_batch in queries.items():\n",
    "\n",
    "    # query-a\n",
    "    obs_a = query_batch['obs_a']\n",
    "    action_a = query_batch['action_a']\n",
    "    policy_a, _ = opcc.get_policy(*policy_a_id)\n",
    "\n",
    "    # query-b\n",
    "    obs_b = query_batch['obs_b']\n",
    "    action_b = query_batch['action_b']\n",
    "    policy_b, _ = opcc.get_policy(*policy_b_id)\n",
    "\n",
    "    # horizon for policy evaluation\n",
    "    horizon = query_batch['horizon']\n",
    "\n",
    "    # ground truth binary vector:\n",
    "    # (Q(obs_a, action_a, policy_a, horizon)\n",
    "    # <  Q(obs_b, action_b, policy_b, horizon))\n",
    "    target = query_batch['target'].tolist()\n",
    "    targets += target\n",
    "\n",
    "    # Let's make predictions for the given queries.\n",
    "    # One can use any mechanism to predict the corresponding\n",
    "    # answer to queries, and we simply use a random predictor\n",
    "    # over here for demonstration purposes\n",
    "    p, c = random_predictor(obs_a, obs_b, action_a, action_b,\n",
    "                            policy_a, policy_b, horizon)\n",
    "    predictions += p\n",
    "    confidences += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the query `predictions` and `targets`, we demo estimation of various evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################\n",
    "# (Section 3.3 in paper)\n",
    "# #########################################\n",
    "loss = np.logical_xor(predictions, targets)  # we use 0-1 loss for demo\n",
    "\n",
    "# List of tuples (coverage, selective_risks, tau)\n",
    "coverage_sr_tau = []\n",
    "tau_interval=0.01\n",
    "for tau in np.arange(0, 1 + 2 * tau_interval, tau_interval):\n",
    "  non_abstain_filter = confidences >= tau\n",
    "  if any(non_abstain_filter):\n",
    "    selective_risk = np.sum(loss[non_abstain_filter])\n",
    "    selective_risk /= np.sum(non_abstain_filter)\n",
    "    coverage = np.mean(non_abstain_filter)\n",
    "    coverage_sr_tau.append((coverage, selective_risk, tau))\n",
    "  else:\n",
    "    # 0 risk for 0 coverage\n",
    "    coverage_sr_tau.append((0, 0, tau))\n",
    "\n",
    "coverages, selective_risks, taus = list(zip(*sorted(coverage_sr_tau)))\n",
    "assert selective_risks[0] == 0 and coverages[0] == 0 , \"no coverage not found\"\n",
    "assert coverages[-1] == 1, 'complete coverage not found'\n",
    "\n",
    "# AURCC ( Area Under Risk-Coverage Curve): Ideally, we would like it to be 0\n",
    "aurcc = metrics.auc(x=coverages,y=selective_risks)\n",
    "\n",
    "# Reverse-pair-proportion\n",
    "rpp = np.logical_and(np.expand_dims(loss, 1)\n",
    "                     < np.expand_dims(loss, 1).transpose(),\n",
    "                     np.expand_dims(confidences, 1)\n",
    "                     < np.expand_dims(confidences, 1).transpose()).mean()\n",
    "\n",
    "# Coverage Resolution (cr_k) : Ideally, we would like it to be 1\n",
    "k = 10\n",
    "bins = [_ for _ in np.arange(0, 1, 1 / k)]\n",
    "cr_k = np.unique(np.digitize(coverages, bins)).size / len(bins)\n",
    "\n",
    "# print evaluation metrics\n",
    "print(f\"aurcc: {aurcc}, rpp: {rpp}, cr_{k}:{cr_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Every environment comes along with multiple datasets ( borrowed from d4rl) and can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# Datasets: (Section 4 in paper - step (1) )\n",
    "# ###########################################\n",
    "env_name = 'HalfCheetah-v2'\n",
    "\n",
    "# list all dataset names corresponding to an env\n",
    "dataset_names = opcc.get_dataset_names(env_name)\n",
    "dataset_name = 'random'\n",
    "\n",
    "# This is a very-slim wrapper over D4RL datasets.\n",
    "dataset = opcc.get_qlearning_dataset(env_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opcc, gym, torch\n",
    "\n",
    "env_name = \"HalfCheetah-v2\"\n",
    "policy, policy_info = opcc.get_policy(env_name, pre_trained=1)\n",
    "\n",
    "done = False\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "episode_score=0\n",
    "\n",
    "while not done:\n",
    "    action = policy(torch.tensor(obs).unsqueeze(0))\n",
    "    action = action.data.cpu().numpy()[0].astype('float32')\n",
    "    obs, reward, done, step_info = env.step(action)\n",
    "    episode_score += reward\n",
    "\n",
    "print(f\"Episode Score: {episode_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
